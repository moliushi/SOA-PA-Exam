---
title: "Exam PA December 2018 Rmd Template"

---

# Load data

Load data provided for project.

```{r}
.libPaths("C:/Users/sam.castillo/Desktop/PA/library/PAlibrary")
library(dplyr)
library(ggplot2)
library(readr)
library(tidyr)
library(broom)
library(forcats)
library(caret)
library(gridExtra)
library(rpart)
library(rpart.plot)
# Read in data files
data.all <- read_csv('MSHA_Mine_Data_2013-2016.csv')

```

# Three useful items

I find these three items useful and so am providing them here in case you have need for what they do. The first and second will not run until some of the code is replaced with actual data freame and/or variable names, but the third, which creates a function, needs to be run in order for the function to be used.

The first uses caret to partition a dataset.

```{r}
library(caret)
set.seed(1234)
partition <- createDataPartition(data.frame$FEATURE, list = FALSE, p = .80)
train <- data.frame[partition, ]
test <- data.frame[-partition, ]
# replace "data.frame" with the name of your dataframe (3 times)
# replace "FEATURE" with the target variable
# The proportion that goes to the training set need not be .80
# Consider giving more intuitive names to the two sets created here
```

The second is how to change the order of the levels for a factor (categorical variable). This can make a difference for GLM results as the first level becomes the baseline and all but the first level become additional predictive variables. In general, for GLMs it is good set the base (reference) level to the one that has the most observations.

```{r}
levels(data.frame$CATEGORICAL)
data.frame$CATEGORICAL <- relevel(data.frame$CATEGORICAL, ref = "Level Name")
levels(data.frame$CATEGORICAL)
# the levels function might help you see the effect of the change
# replace "data.frame" with the name of your dataframe (2 times)
# replace "CATEGORICAL" with the name of a variable that is a factor (categorical variable) (2 times)
# replace "Level Name" with the name of the level that should become the first level
```

The third is a function that measures the quality of fit of predicted versus actual outcomes. It is the loglikelihood function for a Poisson model (up to a constant) and can be used to assess fit against the data used to build the model or against a test set. I will mention the Poisson model later on in this file. As with any loglikelihood function, less negative or more positive values indicate a better fit.

There are some special considerations taken care of by this function. At times when using trees, the predicticted value is slightly negative (a computer issue) when it should be zero. In addition, when the predicted value is zero, the log cannot be taken. If the target is zero, evaluating the loglikehood to zero makes sense. If not, a small value such as 0.000001 can be used. These adjustments are made in the function.

```{r}
LLfunction <- function(targets, predicted_values){
  p_v_zero <- ifelse(predicted_values <= 0, 0, predicted_values)
  p_v_pos <- ifelse(predicted_values <= 0, 0.000001 ,predicted_values)
  return(sum(targets*log(p_v_pos)) - sum(p_v_zero))
}
# "targets" is a vector containing the actual values for the target variable
# "predicted_values" is a vector containing the predicted values for the target variable
eval_model <- function(obs, preds){
  standard_metrics <- postResample(preds, obs) 
  custom_metrics <- LLfunction(obs, preds)
  standard_metrics %>% c(logLik = custom_metrics)
}


```



# Data exploration and cleaning

To get a sense of the data, here are summary statistics:

```{r}
summary(data.all)
```

```{r}
PCT_FEATURES <- df %>% select(contains("PCT")) %>% 
  names()

df <- data.all %>% 
  filter(!(US_STATE %in% c("AS", "GU", "MP", NA)),  #remove states with only a few observations.  These are not even US States
         !is.na(PRIMARY),  #include only records with valid PRIMARY status
         !is.na(MINE_STATUS), #remove NAs
         AVG_EMP_TOTAL < 2000) %>% #remove 4 outlying cases
  mutate(NA_FLAG = case_when(SEAM_HEIGHT == 0 ~ 'SEAM_HEIGHT',
                             T ~ 'NONE'),
         PRIMARY_BUCKET = case_when(PRIMARY == 'Sand & gravel' ~ 'S1',
                                    PRIMARY == 'Limestone, crushed and broken' ~ 'S2',
                                    PRIMARY == 'Coal, Bituminous' ~ 'S3',
                                    PRIMARY == 'Stone, crushed and broken, NEC' ~ 'S4',
                                    T ~ 'OTHER'),
         MINE_STATUS_FIXED  = case_when(MINE_STATUS %in% c("Active", "Full-time permanent") ~ 'OPEN',
                                        MINE_STATUS %in% c('Intermittent', 'Non-producing') ~ 'INTERMITTENT',
                                        MINE_STATUS %in% c("Permanently abandoned", "Temporarily closed", "Closed by MSHA") ~"CLOSED")) %>% 
 mutate_at(PCT_FEATURES, ~.x*EMP_HRS_TOTAL) %>% #this converts all of the percent features (i.e.,PCT_HRS_UNDERGROUND, PCT_HRS_SURFACE, etc ) to actual hours by multiplying by the total employee hours EMP_HRS_TOTAL
  
  select(-MINE_STATUS) %>% 
  mutate_at(c("TYPE_OF_MINE","NA_FLAG", "US_STATE","MINE_STATUS_FIXED", "PRIMARY_BUCKET", "PRIMARY", "COMMODITY"), fct_infreq)#set base factor levels to those with the most observations
df %>% count(YEAR)
df %>% count(US_STATE) %>% arrange(n)
df %>% count(COMMODITY)
df %>% count(PRIMARY) %>% arrange(desc(n))
df %>% count(SEAM_HEIGHT)
df %>% count(TYPE_OF_MINE)

```

Coal miners use different terminology than the other miners for when a mine is closed.  This is fixed.

```{r}
df %>% count(MINE_STATUS, COMMODITY)
```

```{r}
df %>% mutate(MINE_STATUS_FIXED  = case_when(MINE_STATUS %in% c("Active", "Full-time permanent") ~ 'OPEN',
                                        MINE_STATUS %in% c('Intermittent', 'Non-producing') ~ 'INTERMITTENT',
                                        MINE_STATUS %in% c("Permanently abandoned", "Temporarily closed", "Closed by MSHA") ~"CLOSED")) %>% 
  count(MINE_STATUS_FIXED)
```


- There are about the same number of records for each year. 
- There is a lot of variation in the number of records by US state.  PA has 3,501 and smaller states (or US territories) only have one.  
  The following states are being removed because they have fewer than 10 records: AS, GU, MP, NA (which is a missing value)
- There are five different types of mines.  These are Coal, Metal, Nonmetal, Sand & gravel, and Stote.  Most mines are sand and Gravel or Stone with a smaller percentage Coal and Metal.
- We have additional information on the primary extract of the mine.  There was one record with a missing Primary field wihch was removed.
- The SEAM_HEIGHT field is apparently a measure of the mine dimension.  There are 49,823 zero values which indicates that these values are actually missing instead of 0.  I created an addition field called NA_FLAG which is 1 when the SEAM_HEIGHT is 0.  This helps linear models to make sense of the non-linearity of the data.
- There are 13 mines which have a missing MINE_STATUS value.  These are being removed.  If I had more information I would change these to being closed.  Given limited knowledge in this area I am being safe and removing these records.

```{r}
primary_bucket <- df %>% 
  count(PRIMARY) %>% 
  arrange(desc(n)) %>% 
  murate(PRIMARY_BUCKET = case_when(PRIMARY == 'Sand & gravel' ~ 'S1',
                                    PRIMARY == 'Limestone, crushed and broken' ~ 'S2',
                                    PRIMARY == 'Coal, Bituminous' ~ 'S3',
                                    PRIMARY == 'Stone, crushed and broken, NEC' ~ 'S4',
                                    T ~ 'OTHER'))
  
df$PRIMARY %>% unique() %>% length()
```

Removed 27 rows. Not sure how to interpret all the PRIMARY categories given there are so many possibilities, will leave out for now to simplify matters.

This field simplified the PRIMARY field down to a few levels which can be used in the model.  

```{r}
df %>% count(PRIMARY_BUCKET)
```

There appears to be some potential outliers and perhaps some other variables that should be eliminated before building a model. No time for that, but will leave you with one graph, a log-log plot of employees vs hours. After that I will move on set up some models.

Four records were removed where the average employee total hours was greater than 4000

```{r}
data.all %>% 
  ggplot(aes(AVG_EMP_TOTAL, EMP_HRS_TOTAL)) + 
  geom_point(alpha = 0.4, color = "blue") + 
  theme_minimal()
```

The three continuous predictors AVG_EMP_TOTAL, EMP_HRS_TOTAL, and SEAM_HEIGHT were skewed and so a log transform was applied.  After applying the transformation the data distributions are more symmetric which makes them easier to use in modeling.  One of the assumtions of GLMs is that the covariate distributions are at least approximately normal.  It is ok if these are slightly off (such as a t-distribution).

```{r fig.height=5, fig.width=12}
p1 <- df %>% 
  ggplot(aes(log(AVG_EMP_TOTAL))) + 
  geom_histogram() + 
  theme_minimal()

p2 <- df %>% 
  ggplot(aes(log(EMP_HRS_TOTAL))) + 
  geom_histogram()  + 
  theme_minimal()

p3 <- df %>% 
  ggplot(aes(log(SEAM_HEIGHT))) + 
  geom_histogram() +  
  theme_minimal()

grid.arrange(p1, p2, p3, nrow = 1)

```

```{r}
data.all %>% mutate(no_injuries = ifelse(NUM_INJURIES == 0, 'NO INJURIES', 'AT LEAST ONE INJURY')) %>% count(no_injuries) %>% mutate(percent_of_total = round(n/sum(n),1))
```
I checked that all values do actually add up to 1.

```{r}
PCT_FEATURES <- df %>% select(contains("PCT")) %>% 
  names()

df %>% 
  transmute(pct_total = PCT_HRS_UNDERGROUND + PCT_HRS_SURFACE + PCT_HRS_STRIP + PCT_HRS_AUGER + PCT_HRS_CULM_BANK + PCT_HRS_DREDGE + PCT_HRS_OTHER_SURFACE + PCT_HRS_SHOP_YARD + PCT_HRS_MILL_PREP + PCT_HRS_OFFICE) %>% 
  summary()
```

```{r}
df %>% mutate_at(PCT_FEATURES, ~.x*EMP_HRS_TOTAL) %>% 
  select(PCT_FEATURES) %>% 
  summary()
```


# Decision tree

The following code sets up a decision tree using all the variables in the dataframe "data.reduced." It also uses the full dataset. The left side of the formula is employee hours per year followed by number of injuries. Number of injuries is what is being predicted, but employee hours is used as an offset as the number injuries is expected to be proportional to the number of employee hours worked. This formula format automatically results in a Poisson method being used, but I am stating it explicitly for clarity. Need to make sure to remove EMP_HRS_TOTAL from the formula as that is not a predictor variable.

This code sets arbitrary parameters for the control, then prunes the tree. It then calculates the loglikelihood using the entire dataset. I've not had time to work with training and testing sets. When I worked with the data set cleaned as above, the resulting tree was too complex to easily interpret. Perhaps by working further with the data, controls, and pruning, you can come up with a tree that makes sense and can be explained to the union.

```{r}
df_tree <- df %>% select(-PRIMARY, -US_STATE)
train_index <- createDataPartition(df_tree$NUM_INJURIES, p = 0.8, list = F) %>% as.numeric()
df_tree_train <- df_tree %>% dplyr::slice(train_index)
df_tree_test <- df_tree %>% dplyr::slice(-train_index)

df_glm <- df %>% select(-PRIMARY, -TYPE_OF_MINE) 
train_index <- createDataPartition(df_glm$NUM_INJURIES, p = 0.8, list = F) %>% as.numeric()
df_glm_train <- df_glm %>% dplyr::slice(train_index)
df_glm_test <- df_glm %>% dplyr::slice(-train_index)
```


```{r}
library(rpart)
library(rpart.plot)
set.seed(153) # because rpart uses cross-validation for estimating complexity parameter
tree.reduced <- rpart(cbind(EMP_HRS_TOTAL/2000, NUM_INJURIES) ~ . - EMP_HRS_TOTAL,
                      data = df_tree_train,
                      method = "poisson",
                      control = rpart.control(minbucket = 500, 
                                              cp = 0.001, 
                                              maxdepth = 5))
plotcp(tree.reduced)
tree.reduced.pruned <- prune(tree.reduced, 
                             cp = tree.reduced$cptable[which.min(tree.reduced$cptable[, "xerror"]), "CP"])
rpart.plot(tree.reduced.pruned, type = 2)
printcp(tree.reduced.pruned)
tree.reduced.pruned

tree_preds <- predict(tree.reduced.pruned, newdata = df_tree_test, type = "vector")*df_tree_test$EMP_HRS_TOTAL/2000

#pruned.predict <- (data.reduced$EMP_HRS_TOTAL/2000)*predict(tree.reduced.pruned, newdata = data.reduced, type = "vector") # The prediction for the loglikelihood function should be the number of injuries, not the injury rate

```

The fit for the decision tree is reasonable.  The log likelihood is positive and the R^2, RMSE, and MAE are low.  The MAE of 0.41 indicates that on average, the model either over predicts or under-predicts the number of injuries by about 0.4.  This is less than 1 person on average.

```{r}
eval_model(df_tree_test$NUM_INJURIES, tree_preds) 
```

```{r}
tree.reduced <- rpart(cbind(EMP_HRS_TOTAL/2000, NUM_INJURIES) ~ . - EMP_HRS_TOTAL,
                      data = df_tree_train,
                      method = "poisson",
                      control = rpart.control(minbucket = 50, 
                                              cp = 0.01, 
                                              maxdepth = 7))
plotcp(tree.reduced)
tree.reduced.pruned <- prune(tree.reduced, 
                             cp = tree.reduced$cptable[which.min(tree.reduced$cptable[, "xerror"]), "CP"])
rpart.plot(tree.reduced.pruned)
printcp(tree.reduced.pruned)
tree.reduced.pruned

tree_preds_2 <- predict(tree.reduced.pruned, newdata = df_tree_test, type = "vector")*df_tree_test$EMP_HRS_TOTAL/2000

eval_model(df_tree_test$NUM_INJURIES, tree_preds_2)
```

```{r}
tree.reduced <- rpart(cbind(EMP_HRS_TOTAL/2000, NUM_INJURIES) ~ . - EMP_HRS_TOTAL,
                      data = df_tree_train,
                      method = "poisson",
                      control = rpart.control(minbucket = 500, 
                                              cp = 0.001, 
                                              maxdepth = 7))
plotcp(tree.reduced)
tree.reduced.pruned <- prune(tree.reduced, 
                             cp = tree.reduced$cptable[which.min(tree.reduced$cptable[, "xerror"]), "CP"])
rpart.plot(tree.reduced.pruned)
printcp(tree.reduced.pruned)
tree.reduced.pruned

tree_preds_3 <- predict(tree.reduced.pruned, newdata = df_tree_test, type = "vector")*df_tree_test$EMP_HRS_TOTAL/2000

eval_model(df_tree_test$NUM_INJURIES, tree_preds_3)
```

# GLM

The following code produces a poisson GLM. The log link is the default and the offset is a log here because it acts at the level of the linear model. As with the tree, when I ran this using the data I had there were some odd results. There are NAs for "sand & gravel" and something about a rank-deficient fit, which may be tied the huge coeffients for the hours variable and with the fact that they sum to 1 in each case.


```{r}
glm.reduced <- glm(NUM_INJURIES ~ . - EMP_HRS_TOTAL,
                   family = poisson(),
                   offset = log(EMP_HRS_TOTAL/2000),
                   data = df_glm_train)
summary(glm.reduced)

glm.predict <- predict(glm.reduced, newdata = df_glm_test, type = "response")
# For GLM with an offset, this predict function includes the effect of the offset, producing the number of injuries.
print("loglikelihood")
glm_0_metrics <- eval_model(df_glm_test$NUM_INJURIES, glm.predict)
```

The fit for your GLM is not good.  The log likelihood is negative.  This is because the coefficients on the percentages are all large.

```{r}
glm_0_metrics
```

# Final models

Once analysis is done in above sections, run the final models on all data here (even if training and test sets were previously used) so it is clear how to make the connection between the code above and what is produced for the report.

```{r}
glm <- glm(NUM_INJURIES ~ . - EMP_HRS_TOTAL,
                   family = poisson(),
                   offset = log(EMP_HRS_TOTAL/2000),
                   data = df_glm_train)
summary(glm)

glm.predict <- predict(glm, newdata = df_glm_test, type = "response")
# For GLM with an offset, this predict function includes the effect of the offset, producing the number of injuries.
print("loglikelihood")
glm_1_metrics <- eval_model(df_glm_test$NUM_INJURIES, glm.predict)
glm_1_metrics
plot(glm)
```

TYPE_OF_MINEMill is producing NA values and so remove it or simplify level

```{r}
names(df_glm)
```
A log transform is applied to the continuous predictors SEAM_HEIGHT and EMP_HRS_TOTAL.

```{r}
glm <- glm(NUM_INJURIES ~ YEAR + US_STATE + COMMODITY + log(SEAM_HEIGHT + 1)  + log(EMP_HRS_TOTAL) +  PCT_HRS_UNDERGROUND + PCT_HRS_SURFACE + PCT_HRS_STRIP + PCT_HRS_AUGER + PCT_HRS_CULM_BANK + PCT_HRS_DREDGE + PCT_HRS_OTHER_SURFACE + PCT_HRS_SHOP_YARD + PCT_HRS_MILL_PREP + PCT_HRS_OFFICE,
                   family = poisson(),
                   offset = log(EMP_HRS_TOTAL/2000),
                   data = df_glm_train)
summary(glm)

glm.predict <- predict(glm, newdata = df_glm_test, type = "response")
# For GLM with an offset, this predict function includes the effect of the offset, producing the number of injuries.
print("loglikelihood")
glm_2_metrics <- eval_model(df_glm_test$NUM_INJURIES, glm.predict)
glm_2_metrics
plot(glm)
```

**Note: although these fields are NAMED "PCT_HRS_###" they are actually in unites of total hours.  This is just shorthand to save time.**

- Removing TYPE_OF_MINE
- Applying log + 1 transform to PCT_HOURS variales which are really in units of hours

```{r}
glm <- glm(NUM_INJURIES ~ YEAR  + COMMODITY + SEAM_HEIGHT + EMP_HRS_TOTAL +  PCT_HRS_UNDERGROUND + PCT_HRS_SURFACE + PCT_HRS_STRIP +PCT_HRS_AUGER + PCT_HRS_CULM_BANK + PCT_HRS_DREDGE + PCT_HRS_OTHER_SURFACE + PCT_HRS_SHOP_YARD + PCT_HRS_MILL_PREP + PCT_HRS_OFFICE,
                   family = poisson(),
                   offset = log(EMP_HRS_TOTAL/2000),
                   data = df_glm_train)
summary(glm)

glm.predict <- predict(glm, newdata = df_glm_test, type = "response")
# For GLM with an offset, this predict function includes the effect of the offset, producing the number of injuries.
print("loglikelihood")
glm_3_metrics <- eval_model(df_glm_test$NUM_INJURIES, glm.predict)
glm_3_metrics
plot(glm)
```


This model is run over the entire data set

```{r}
glm_final <- glm(NUM_INJURIES ~ YEAR + COMMODITY + SEAM_HEIGHT + EMP_HRS_TOTAL +  PCT_HRS_UNDERGROUND + PCT_HRS_SURFACE + PCT_HRS_STRIP +PCT_HRS_AUGER + PCT_HRS_CULM_BANK + PCT_HRS_DREDGE + PCT_HRS_OTHER_SURFACE + PCT_HRS_SHOP_YARD + PCT_HRS_MILL_PREP + PCT_HRS_OFFICE,
                   family = poisson(),
                   offset = log(EMP_HRS_TOTAL/2000),
                   data = df_glm) #use ALL data

summary(glm_final)
tidy(glm_final) %>% write_csv("C:/Users/sam.castillo/Desktop/PA/Prior Exam/final_glm.csv")

glm.predict <- predict(glm, newdata = df_glm, type = "response")
# For GLM with an offset, this predict function includes the effect of the offset, producing the number of injuries.
print("loglikelihood")
glm_final_metrics <- eval_model(df_glm_test$NUM_INJURIES, glm.predict)
```

```{r}
rbind(glm_0_metrics,glm_1_metrics, glm_2_metrics, glm_3_metrics)
```

